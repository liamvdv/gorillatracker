# !!!
# NOTE: DO NOT EDIT THIS YAML FILE. INSTEAD, COPY IT TO A NEW FILE AND EDIT THAT FILE.
# !!!

# For more details see src/gorillatracker/args.py

# Device Arguments

accelerator: cuda               # Device accelerator (e.g., "cuda" for GPU)
num_devices: 1                  # Number of devices (GPUs) to use
distributed_strategy: "auto"    # Distributed training strategy (if applicable)
force_deterministic: True      # Force deterministic behavior
precision: 32                   # Training precision (e.g., "bf16-mixed")
compile: False                  # Compile the model for faster execution
workers: 4                      # Number of workers for data loading


# Model and Training Arguments

project_name: ""                # WandB project name
run_name: ""                    # Name for this training run
wandb_tags: ["test"]            # WandB tags for experiment tracking
model_name_or_path: "timm/vit_base_patch14_dinov2.lvd142m" # Model name or path (e.g. Timm path)
saved_checkpoint_path: Null     # Path to a saved model checkpoint (if resuming)
freeze_backbone: False          # Freeze the backbone
resume: False                   # Resume training from a saved checkpoint can also be a wandb path (wandb:model_id:tag)
fast_dev_run: False             # Enable fast development run
profiler: Null                  # Profiler to use (e.g., "simple", "advanced", "pytorch")
offline: False                  # Enable offline mode for WandB
data_preprocessing_only: False  # Run only data preprocessing
seed: 42                        # Random seed for reproducibility
debug: False                    # Enable debug mode
from_scratch: False             # Train the model from scratch
early_stopping_patience: 3      # Early stopping patience (number of epochs)
min_delta: 0.01                 # Early stopping delta
embedding_size: 128
dropout_p: 0.32
embedding_id: "linear"          # Embedding layer type (e.g., "linear", "mlp", ...)
pool_mode: None                 # Pooling mode
fix_img_size: None              # Fix image size (only for timm transformer models)
use_quantization_aware_training: False

# Optimizer Arguments
weight_decay: 0.1               # Weight decay
beta1: 0.9                      # Adam optimizer's beta1 parameter
beta2: 0.999                    # Adam optimizer's beta2 parameter
epsilon: 1e-7                   # Adam optimizer's epsilon


# L2SP Arguments (only applied if loss uses L2SP)
l2_alpha: 0.1                       # Alpha for the l2sp loss
l2_beta: 0.01                       # Beta for the l2sp loss
path_to_pretrained_weights: ""      # Path to the pretrained weights for the l2sp loss

# LR Arguments
lr_schedule: "constant"           # Learning rate schedule (e.g., "linear", "cosine", "exponential", "reduce_on_plateau") TODO: add 
warmup_mode: "constant"           # Warmup mode (e.g., "linear", "cosine", "exponential", "constant")
warmup_epochs: 0                # Number of warmup epochs (if 0 no warmup is performed)
initial_lr: 1e-5                # Initial learning rate before warmup(must be > 0.0)
start_lr: 1e-5                  # Learning Rate after warmup at the beginning of 'normal' scheduling
end_lr: 1e-5                    # End learning rate (for the learning rate schedule -> cosine learning rate schedule)
stepwise_schedule: False        # Use stepwise schedule with lr change after each validation
lr_interval : 1                 # Interval for lr change in stepwise schedule / Fraction of epoch 

# Loss Arguments
margin: 1.0                     # Margin for the contrastive loss (triplet loss)
s: 64.0                         # Scale for the softmax losses 
loss_mode: "offline/native"      # Loss modes are "offline", "offline/native", "online/soft", "online/semi-hard", "online/hard", "softmax/arcface"
loss_dist_term: "euclidean"        # Distance term for the loss function

batch_size: 512                 # Training batch size
grad_clip: 1.0                  # Gradient clipping value
gradient_accumulation_steps: 4  # Gradient accumulation steps
max_epochs: 5                   # Training goal (large number)
val_check_interval: 1.0         # Validation check interval as a fraction of total steps or as an fixed number of steps
val_before_training: True      # Perform validation before training
only_val: False                 # Perform only validation
kfold: False                     # Perform kfold cross validation

save_interval: 2                # Model checkpoint save interval as a fraction of total steps
embedding_save_interval: 2      # Embedding save interval
save_model_to_wandb: False      # Save the model to wandb

# Data
dataset_class: "gorillatracker.datasets.mnist.MNISTDataset" # Dataset class
data_dir: "/workspaces/gorillatracker/data/splits/dataset"  # Data directory
additional_val_dataset_classes: [gorillatracker.datasets.mnist.MNISTDataset] # Dataset classes for additional validation
additional_val_data_dirs: ["/workspaces/gorillatracker/data/splits/dataset"] # Data directories for additional validation
dataset_names: ["train_data", "val_data_1"]                 # names for used datasets
data_resize_transform: Null

# SSL
use_ssl: False                # use ssl
tff_selection: "equidistant"  # Tracking Frame Feature selection (which images should be used of a tracklet)
split_path: "/workspaces/gorillatracker/data/ssl_splits/split1" # which data split should be used
negative_mining: "overlapping"  # which tracklets should be used as negatives
n_samples: 15                 # number of samples per tracklet
feature_types: ["face45"]     # which features should be used for training
min_confidence: 0.7           # min detection confidence for features
min_images_per_tracking: 5    # only tracklets with >x images are used in training