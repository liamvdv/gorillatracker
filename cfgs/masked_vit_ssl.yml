# Device Arguments

accelerator: cuda               # Device accelerator (e.g., "cuda" for GPU)
precision: 32                   # Training precision (e.g., "bf16-mixed")
workers: 40                      # Number of workers for data loading


# Model and Training Arguments

project_name: "Embedding-ViTLarge-MAE-Face" # WandB project name form: <Function>-<Backbone>-<Dataset>-<Set-Type> eg. Embedding-ResNet50-CXL-Open
run_name: "500-add-mae" # Name for this training run form: <Issue Number>-<Purpose> eg. #123-testing-new-data
model_name_or_path: "MaskedVisionTransformer"  # Model name or path 
fast_dev_run: False               # Enable fast development run
offline: False                    # Enable offline mode for WandB
data_preprocessing_only: False    # Run only data preprocessing
early_stopping_patience: 10        # Early stopping patience (number of epochs)
embedding_size: 1024

# Optimizer Arguments
# weight_decay: 0.2               # Weight decay if l2sp regularisation is used this is set to 0.0
beta1: 0.9                      # Adam optimizer's beta1 parameter
beta2: 0.999                    # Adam optimizer's beta2 parameter
epsilon: 1e-7                   # Adam optimizer's epsilon

# L2SP Arguments
l2_alpha: 0.01                       # Alpha for the l2sp loss
l2_beta: 0.01                     # Beta for the l2sp loss
path_to_pretrained_weights: "pretrained_weights/vit_large_dinov2_2.pth"    # Path to the pretrained weights for the l2sp loss

loss_mode: "mae_mse/l2sp"         # Loss modes are "offline", "offline/native", "online/soft", "online/semi-hard", "online/hard", "softmax/arcface"
                                    # Each loss is availible with l2sp regularisation just add /l2sp to the loss mode


lr_schedule: "cosine"           # Learning rate schedule (e.g., "linear", "cosine", "exponential", "reduce_on_plateau") TODO: add 
warmup_mode: "constant"           # Warmup mode (e.g., "linear", "cosine", "exponential", "constant")
warmup_epochs: 0                # Number of warmup epochs (if 0 no warmup is performed)
initial_lr: 1e-4                # Initial learning rate before warmup(must be > 0.0)
start_lr: 1e-4                  # Learning Rate after warmup at the beginning of 'normal' scheduling
end_lr: 1e-7                    # End learning rate (for the learning rate schedule -> cosine learning rate schedule)
stepwise_schedule: True


batch_size: 256                  # Training batch size
grad_clip: 1.0                  # Gradient clipping value
gradient_accumulation_steps: 4  # Gradient accumulation steps
max_epochs: 100                  # Training goal (large number)
val_before_training: True       # Perform validation before training
only_val: False                 # Perform only validation

val_check_interval: 2.0
save_interval: 2                # Model checkpoint save interval as a fraction of total steps
embedding_save_interval: 1      # Embedding save interval
knn_with_train: False           

# Config and Data Arguments
# wandb_tags: ["research-template"] # WandB tags for experiment tracking
use_ssl: True
split_path: "/workspaces/gorillatracker/data/splits/SSL/SSL-10k-100-1000_2024-04-18_percentage-90-5-5_split_20240619_0955.pkl"
feature_types: ["face_90"]
n_samples: 5
# width_range: [224, 1920]
# height_range: [224, 1080]

data_resize_transform: 224

additional_val_dataset_classes: [gorillatracker.datasets.cxl.CXLDataset, gorillatracker.datasets.bristol.BristolDataset, gorillatracker.datasets.cxl.CXLDataset]
additional_val_data_dirs: ["/workspaces/gorillatracker/data/supervised/cxl_all/face_images_square", "/workspaces/gorillatracker/data/supervised/bristol/cross_encounter_validation/cropped_frames_square_filtered", "/workspaces/gorillatracker/data/supervised/splits/cxl_faces_openset_seed_42_square/val"]
dataset_names: ["ssl", "cxl-all", "bristol", "cxl-val"]
dataset_class: "gorillatracker.datasets.ssl.SSLDataset"
data_dir: "/workspaces/gorillatracker/cropped-images/2024-04-18"