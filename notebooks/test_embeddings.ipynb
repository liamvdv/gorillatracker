{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# load model from wandb artifact\n",
    "def load_model_from_artifact(artifact: wandb.Artifact, model: Any, device: str) -> Any:  # TODO\n",
    "    model_state_dict = artifact.get(\"model_state_dict\").state_dict()\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "# generate embeddings (dataframe) from model and dataset\n",
    "def generate_embeddings(model, dataset, device):  # TODO\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    df = pd.DataFrame(columns=[\"embedding\", \"label\", \"input\", \"label_string\"])\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in tqdm(dataset):  # TODO\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            embeddings = model(batch_inputs)\n",
    "            for i in range(len(batch_inputs)):\n",
    "                df.append(\n",
    "                    {\n",
    "                        \"embedding\": embeddings[i],\n",
    "                        \"label\": batch_labels[i],\n",
    "                        \"input\": batch_inputs[i],\n",
    "                        \"label_string\": dataset.mapping[batch_labels[i]],\n",
    "                    },\n",
    "                    ignore_index=True,\n",
    "                    inplace=True,\n",
    "                )\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code used for loading in another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.init(mode=\"disabled\")\n",
    "api = wandb.Api()\n",
    "\n",
    "artifact = api.artifact(\n",
    "    \"gorillas/Embedding-ALL-SPAC-Open/model-3ag1c2vf:v1\",  # your artifact name\n",
    "    type=\"model\",\n",
    ")\n",
    "artifact_dir = artifact.download()\n",
    "model = artifact_dir + \"/model.ckpt\"\n",
    "\n",
    "# load model\n",
    "checkpoint = torch.load(model, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "model = EfficientNetV2Wrapper(  # switch this with the model you want to use\n",
    "    model_name_or_path=\"EfficientNetV2_Large\",\n",
    "    from_scratch=False,\n",
    "    loss_mode=\"softmax/arcface\",\n",
    "    weight_decay=0.001,\n",
    "    lr_schedule=\"cosine\",\n",
    "    warmup_mode=\"cosine\",\n",
    "    warmup_epochs=10,\n",
    "    max_epochs=100,\n",
    "    initial_lr=0.01,\n",
    "    start_lr=0.01,\n",
    "    end_lr=0.0001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    embedding_size=128,\n",
    ")\n",
    "# the following lines are necessary to load a model that was trained with arcface (the prototypes are saved in the state dict)\n",
    "model.loss_module_train.prototypes = torch.nn.Parameter(checkpoint[\"state_dict\"][\"loss_module_train.prototypes\"])\n",
    "model.loss_module_val.prototypes = torch.nn.Parameter(checkpoint[\"state_dict\"][\"loss_module_val.prototypes\"])\n",
    "\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# generate table that contains labels and images and embeddings\n",
    "df = pd.DataFrame(columns=[\"label\", \"image\", \"embedding\"])\n",
    "dataset = CXLDataset(\n",
    "    data_dir=\"/workspaces/gorillatracker/data/splits/ground_truth-cxl-face_images-openset-reid-val-0-test-0-mintraincount-3-seed-42-train-50-val-25-test-25\",\n",
    "    partition=\"val\",\n",
    "    transform=transforms.Compose(  # use the transforms that were used for the model (except of course data augmentations)\n",
    "        [\n",
    "            SquarePad(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    image_tensor, label = dataset[i]\n",
    "    label_string = dataset.mapping[label]\n",
    "    image = transforms.ToPILImage()(image_tensor)\n",
    "    image_tensor = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225])(\n",
    "        image_tensor\n",
    "    )  # if your model was trained with normalization, you need to normalize the images here as well\n",
    "    embedding = model(image_tensor.unsqueeze(0))\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"label_string\": [label_string],\n",
    "                    \"label\": [label],\n",
    "                    \"image\": [image],\n",
    "                    \"embedding\": [embedding[0].detach().numpy()],\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"\\rprocessed {i} images\")\n",
    "df = df.reset_index(drop=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
