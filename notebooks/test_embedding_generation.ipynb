{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gorillatracker.args import TrainingArgs\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Literal, Tuple, Type, Union\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import cv2\n",
    "import cv2.typing as cvt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gorillatracker.model.get_model_cls import get_model_cls\n",
    "from gorillatracker.model.base_module import BaseModule\n",
    "from gorillatracker.train_utils import get_dataset_class\n",
    "from gorillatracker.type_helper import Label\n",
    "from gorillatracker.scripts.create_dataset_from_videos import _crop_image\n",
    "\n",
    "wandbRun = Any\n",
    "\n",
    "\n",
    "def _get_frames_for_ids(json_path: str) -> Any:\n",
    "    \"\"\"Get the frames for the given IDs.\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the JSON file containing the IDs.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of IDs to frames.\n",
    "    \"\"\"\n",
    "    id_frames: Any = {}\n",
    "    face_class: int = 1\n",
    "    # read the JSON file\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    for frame_idx, frame in enumerate(data[\"labels\"]):\n",
    "        for bbox in frame:\n",
    "            if bbox[\"class\"] != face_class:\n",
    "                continue\n",
    "            id = int(bbox[\"id\"])\n",
    "            if id not in id_frames:\n",
    "                id_frames[id] = []\n",
    "            id_frames[id].append((frame_idx, (bbox[\"center_x\"], bbox[\"center_y\"], bbox[\"w\"], bbox[\"h\"])))\n",
    "\n",
    "    return id_frames\n",
    "\n",
    "\n",
    "def get_wandb_api() -> wandb.Api:\n",
    "    if not hasattr(get_wandb_api, \"api\"):\n",
    "        get_wandb_api.api = wandb.Api()  # type: ignore\n",
    "    return get_wandb_api.api  # type: ignore\n",
    "\n",
    "\n",
    "def parse_wandb_url(url: str) -> Tuple[str, str, str]:\n",
    "    assert url.startswith(\"https://wandb.ai/\")\n",
    "    parsed = urlparse(url)\n",
    "    assert parsed.netloc == \"wandb.ai\"\n",
    "    print(parsed, parsed.path.split(\"/\"), parsed.path)\n",
    "    parts = parsed.path.strip(\"/\").split(\n",
    "        \"/\"\n",
    "    )  # ['gorillas', 'Embedding-SwinV2-CXL-Open', 'runs', 'fnyvl65k', 'overview']\n",
    "    entity, project, s_runs, run_id, *rest = parts\n",
    "    assert (\n",
    "        s_runs == \"runs\"\n",
    "    ), \"expect: https://wandb.ai/gorillas/Embedding-SwinV2-CXL-Open/runs/fnyvl65k/overview like format.\"\n",
    "    return entity, project, run_id\n",
    "\n",
    "\n",
    "def get_run(url: str) -> wandbRun:\n",
    "    # https://docs.wandb.ai/ref/python/run\n",
    "    entity, project, run_id = parse_wandb_url(url)\n",
    "    run = get_wandb_api().run(f\"{entity}/{project}/{run_id}\")  # type: ignore\n",
    "    return run\n",
    "\n",
    "\n",
    "def load_model_from_wandb(\n",
    "    wandb_fullname: str, model_cls: Type[BaseModule], model_config: Dict[str, Any], device: str = \"cpu\"\n",
    ") -> BaseModule:\n",
    "    api = get_wandb_api()\n",
    "\n",
    "    artifact = api.artifact(  # type: ignore\n",
    "        wandb_fullname,\n",
    "        type=\"model\",\n",
    "    )\n",
    "    artifact_dir = artifact.download()\n",
    "    model = artifact_dir + \"/model.ckpt\"  # all of our models are saved as model.ckpt\n",
    "    checkpoint = torch.load(model, map_location=torch.device(\"cpu\"))\n",
    "    model_state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "    model = model_cls(**model_config)\n",
    "\n",
    "    if (\n",
    "        \"loss_module_train.prototypes\" in model_state_dict or \"loss_module_val.prototypes\" in model_state_dict\n",
    "    ):  # necessary because arcface loss also saves prototypes\n",
    "        model.loss_module_train.prototypes = torch.nn.Parameter(model_state_dict[\"loss_module_train.prototypes\"])\n",
    "        model.loss_module_val.prototypes = torch.nn.Parameter(model_state_dict[\"loss_module_val.prototypes\"])\n",
    "    # note the following lines can fail if your model was not trained with the same 'embedding structure' as the current model class\n",
    "    # easiest fix is to just use the old embedding structure in the model class\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_embeddings(model: BaseModule, dataset: Any, device: str = \"cpu\") -> pd.DataFrame:\n",
    "    embeddings = []\n",
    "    df = pd.DataFrame(columns=[\"embedding\", \"label\", \"input\", \"label_string\"])\n",
    "    with torch.no_grad():\n",
    "        print(\"Generating embeddings...\")\n",
    "        for imgs, labels in tqdm(dataset):\n",
    "            if isinstance(imgs, torch.Tensor):\n",
    "                imgs = [imgs]\n",
    "                labels = [labels]\n",
    "            batch_inputs = torch.stack(imgs)\n",
    "            if batch_inputs.shape[0] != 1:\n",
    "                batch_inputs = batch_inputs.unsqueeze(1)\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            embeddings = model(batch_inputs)\n",
    "\n",
    "            for i in range(len(imgs)):\n",
    "                input_img = transforms.ToPILImage()(batch_inputs[i].cpu())\n",
    "                df = pd.concat(\n",
    "                    [\n",
    "                        df,\n",
    "                        pd.DataFrame(\n",
    "                            {\n",
    "                                \"embedding\": [embeddings[i]],\n",
    "                                \"label\": [labels[i]],\n",
    "                                \"input\": [input_img],\n",
    "                                \"label_string\": [dataset.mapping[labels[i]]] if dataset.mapping else None,\n",
    "                            }\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    model: BaseModule,\n",
    "    partition: Literal[\"train\", \"val\", \"test\"],\n",
    "    data_dir: str,\n",
    "    dataset_class: str,\n",
    "    transform: Union[Callable[..., Any], None] = None,\n",
    ") -> Dataset[Tuple[Any, Label]]:\n",
    "    cls = get_dataset_class(dataset_class)\n",
    "    if transform is None:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                cls.get_transforms(),  # type: ignore\n",
    "                model.get_tensor_transforms(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return cls(  # type: ignore\n",
    "        data_dir=data_dir,\n",
    "        partition=partition,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_latest_model_checkpoint(run: wandbRun) -> wandb.Artifact:\n",
    "    models = [a for a in run.logged_artifacts() if a.type == \"model\"]\n",
    "    return max(models, key=lambda a: a.created_at)\n",
    "\n",
    "\n",
    "def generate_embeddings_from_run(run_url: str, outpath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    generate a pandas df that generates embeddings for all images in the dataset partitions train and val.\n",
    "    stores to DataFrame\n",
    "    partition, image_path, embedding, label, label_string\n",
    "    \"\"\"\n",
    "    out = Path(outpath)\n",
    "    is_write = outpath != \"-\"\n",
    "    if is_write:\n",
    "        assert not out.exists(), \"outpath must not exist\"\n",
    "        assert out.parent.exists(), \"outpath parent must exist\"\n",
    "        assert out.suffix == \".pkl\", \"outpath must be a pickle file\"\n",
    "\n",
    "    run = get_run(run_url)\n",
    "    print(\"Using model from run:\", run.name)\n",
    "    print(\"Config:\", run.config)\n",
    "    # args = TrainingArgs(**run.config) # NOTE(liamvdv): contains potenially unknown keys / missing keys (e. g. l2_beta)\n",
    "    args = {\n",
    "        k: run.config[k]\n",
    "        for k in (\n",
    "            # Others:\n",
    "            \"model_name_or_path\",\n",
    "            \"dataset_class\",\n",
    "            \"data_dir\",\n",
    "            # Model Params:\n",
    "            \"embedding_size\",\n",
    "            \"from_scratch\",\n",
    "            \"loss_mode\",\n",
    "            \"weight_decay\",\n",
    "            \"lr_schedule\",\n",
    "            \"warmup_mode\",\n",
    "            \"warmup_epochs\",\n",
    "            \"max_epochs\",\n",
    "            \"initial_lr\",\n",
    "            \"start_lr\",\n",
    "            \"end_lr\",\n",
    "            \"beta1\",\n",
    "            \"beta2\",\n",
    "            # NOTE(liamvdv): might need be extended by other keys if model keys change\n",
    "        )\n",
    "    }\n",
    "\n",
    "    print(\"Loading model from latest checkpoint\")\n",
    "    model_path = get_latest_model_checkpoint(run).qualified_name\n",
    "    model_cls = get_model_cls(args[\"model_name_or_path\"])\n",
    "    model = load_model_from_wandb(model_path, model_cls=model_cls, model_config=args)\n",
    "\n",
    "    train_dataset = get_dataset(\n",
    "        partition=\"train\", data_dir=args[\"data_dir\"], model=model, dataset_class=args[\"dataset_class\"]\n",
    "    )\n",
    "    val_dataset = get_dataset(\n",
    "        partition=\"val\", data_dir=args[\"data_dir\"], model=model, dataset_class=args[\"dataset_class\"]\n",
    "    )\n",
    "\n",
    "    val_df = generate_embeddings(model, val_dataset)\n",
    "    val_df[\"partition\"] = \"val\"\n",
    "\n",
    "    train_df = generate_embeddings(model, train_dataset)\n",
    "    train_df[\"partition\"] = \"train\"\n",
    "\n",
    "    df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "    print(\"Embeddings for\", len(df), \"images generated\")\n",
    "\n",
    "    # store\n",
    "    if is_write:\n",
    "        df.to_pickle(outpath)\n",
    "    print(\"done\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_embeddings_from_tracked_video(\n",
    "    model: BaseModule, video_path: str, tracking_data, model_transforms=lambda x: x\n",
    ") -> pd.DataFrame:  # TODO\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: The model to use for embedding generation.\n",
    "        video_path: Path to the video.\n",
    "        tracking_data: Dictionary of Individual IDs to frames. -> {id: List[(frame_idx, (bbox))]} (bbox = (x, y, w, h)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: invididual_id, frame_id, bbox, embedding,\n",
    "    \"\"\"\n",
    "    min_frames = 15  # discard if less than 5 images\n",
    "    max_per_individual = 15\n",
    "\n",
    "    tracking_data = {\n",
    "        id: frames for id, frames in tracking_data.items() if len(frames) >= min_frames\n",
    "    }  # discard if less than 5 images\n",
    "    print(\"Using\", len(tracking_data), \"individuals\")\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    embedding_img_table = pd.DataFrame(columns=[\"embedding\", \"frame_id\", \"bbox\", \"invididual_id\"])\n",
    "\n",
    "    for id, frames in tracking_data.items():\n",
    "        step_size = len(frames) // max_per_individual\n",
    "        if step_size == 0:\n",
    "            continue\n",
    "        frame_list = [frames[i] for i in range(0, max_per_individual * step_size, step_size)]\n",
    "        for frame_idx, bbox in frame_list:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            frame = video.read()[1]  # read the frame. read() returns a tuple of (success, frame)\n",
    "            embedding = get_embedding_from_frame(model, frame, bbox, model_transforms)\n",
    "            embedding_img_table = pd.concat(\n",
    "                [\n",
    "                    embedding_img_table,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"invididual_id\": [id],\n",
    "                            \"frame_id\": [frame_idx],\n",
    "                            \"bbox\": [bbox],\n",
    "                            \"embedding\": [embedding],\n",
    "                        }\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    video.release()\n",
    "    embedding_img_table.reset_index(drop=False, inplace=True)\n",
    "    return embedding_img_table\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_embedding_from_frame(model: BaseModule, frame: cvt.MatLike, bbox, model_transforms) -> torch.Tensor:  # TODO\n",
    "    frame_cropped = _crop_image(\n",
    "        frame,\n",
    "        bbox[0],  # x\n",
    "        bbox[1],  # y\n",
    "        bbox[2],  # w\n",
    "        bbox[3],  # h\n",
    "    )\n",
    "\n",
    "    # convert to pil image\n",
    "    img = cv2.cvtColor(frame_cropped, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    img = model_transforms(img)\n",
    "\n",
    "    model.eval()\n",
    "    embedding = model(img.unsqueeze(0))\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def read_embeddings_from_disk(path: str) -> pd.DataFrame:\n",
    "    return pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code used for loading in another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gorillatracker.model.wrappers_supervised import SwinV2LargeWrapper\n",
    "from gorillatracker.transform_utils import SquarePad\n",
    "from torchvision.transforms import v2 as transforms_v2\n",
    "\n",
    "model_config = {\n",
    "    \"embedding_size\": 128,\n",
    "    \"from_scratch\": False,\n",
    "    \"loss_mode\": \"softmax/arcface\",\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"lr_schedule\": \"cosine\",\n",
    "    \"warmup_mode\": \"linear\",\n",
    "    \"warmup_epochs\": 10,\n",
    "    \"max_epochs\": 100,\n",
    "    \"initial_lr\": 0.01,\n",
    "    \"start_lr\": 0.01,\n",
    "    \"end_lr\": 0.0001,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.999,\n",
    "    \"model_name_or_path\": \"SwinV2LargeWrapper\",\n",
    "    \"stepwise_schedule\": True,\n",
    "    \"lr_interval\": 10,\n",
    "    \"l2_beta\": 0.0,\n",
    "    \"l2_alpha\": 0.0,\n",
    "    \"path_to_pretrained_weights\": \"a/b/c\",\n",
    "}\n",
    "model = load_model_from_wandb(\n",
    "    \"gorillas/Embedding-SwinV2Large-CXL-Open/model-a4t93htr:v14\", SwinV2LargeWrapper, model_config, \"cpu\"\n",
    ")\n",
    "model.eval()\n",
    "model_transforms = transforms.Compose(\n",
    "    [\n",
    "        SquarePad(),\n",
    "        # Uniform input, you may choose higher/lower sizes.\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((192), antialias=True),\n",
    "        transforms_v2.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "video_path = \"/workspaces/gorillatracker/video_data/M002_20220725_006.mp4\"\n",
    "tracked_video_path = (\n",
    "    \"/workspaces/gorillatracker/data/derived_data/spac_gorillas_converted_labels_tracked/M002_20220725_006_tracked.json\"\n",
    ")\n",
    "tracked_video = _get_frames_for_ids(tracked_video_path)\n",
    "embedding_img_table = generate_embeddings_from_tracked_video(model, video_path, tracked_video, model_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_img_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot embeddings using t-SNE use one color for each individual_id\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=10)\n",
    "X = torch.stack(embedding_img_table.embedding.to_list()).numpy().reshape(-1, 128)\n",
    "print(X.shape)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "embedding_img_table[\"tsne-2d-one\"] = X_2d[:, 0]\n",
    "embedding_img_table[\"tsne-2d-two\"] = X_2d[:, 1]\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\",\n",
    "    y=\"tsne-2d-two\",\n",
    "    hue=\"invididual_id\",\n",
    "    palette=sns.color_palette(\"tab10\", len(embedding_img_table.invididual_id.unique())),\n",
    "    data=embedding_img_table,\n",
    "    legend=\"full\",\n",
    "    alpha=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a frame for each individual_id\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, len(embedding_img_table.invididual_id.unique()), figsize=(20, 10))\n",
    "for i, id in enumerate(embedding_img_table.invididual_id.unique()):\n",
    "    img = embedding_img_table[embedding_img_table.invididual_id == id].iloc[10].frame_id\n",
    "    bbox = embedding_img_table[embedding_img_table.invididual_id == id].iloc[0].bbox\n",
    "    # get the frame from the video\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, img)\n",
    "    img = video.read()[1]\n",
    "    img = _crop_image(img, bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "    video.release()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    # crop image\n",
    "    ax[i].imshow(img)\n",
    "    ax[i].set_title(f\"Individual {id}\")\n",
    "    ax[i].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge individual 19 20 and 67\n",
    "\n",
    "embedding_img_table[\"invididual_id\"] = embedding_img_table[\"invididual_id\"].replace({28: 19})\n",
    "embedding_img_table[\"invididual_id\"] = embedding_img_table[\"invididual_id\"].replace({67: 19})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
