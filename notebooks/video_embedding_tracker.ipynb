{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-8vymlbht:v3, 996.45MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from gorillatracker.model import EfficientNetV2Wrapper, SwinV2BaseWrapper\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(mode=\"disabled\")\n",
    "api = wandb.Api()\n",
    "\n",
    "artifact = api.artifact(\n",
    "    \"gorillas/Embedding-SwinV2-CXL-Open/model-8vymlbht:v3\",\n",
    "    type=\"model\",\n",
    ")\n",
    "artifact_dir = artifact.download()\n",
    "model = artifact_dir + \"/model.ckpt\"\n",
    "\n",
    "# load model\n",
    "checkpoint = torch.load(model, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "model = SwinV2BaseWrapper(  # switch this with the model you want to use\n",
    "    model_name_or_path=\"SwinV2_Base\",\n",
    "    from_scratch=False,\n",
    "    loss_mode=\"offline/native\",\n",
    "    weight_decay=0.0001,\n",
    "    lr_schedule=\"linear\",\n",
    "    warmup_mode=\"linear\",\n",
    "    warmup_epochs=0,\n",
    "    max_epochs=10,\n",
    "    initial_lr=0.00001,\n",
    "    start_lr=0.00001,\n",
    "    end_lr=0.00001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    embedding_size=128,\n",
    ")\n",
    "# the following lines are necessary to load a model that was trained with arcface (the prototypes are saved in the state dict)\n",
    "#model.loss_module_train.prototypes = torch.nn.Parameter(checkpoint[\"state_dict\"][\"loss_module_train.prototypes\"])\n",
    "#model.loss_module_val.prototypes = torch.nn.Parameter(checkpoint[\"state_dict\"][\"loss_module_val.prototypes\"])\n",
    "\n",
    "transform=transforms.Compose(  # use the transforms that were used for the model (except of course data augmentations)\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((192, 192)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225]), # if your model was trained with normalization, you need to normalize the images here as well\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Video (and look at all the gorilla ids which contain face images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/gorillatracker/video_data/M002_20220529_031.mp4\n",
      "0\n",
      "[TrackedFrame(f=13, bb=((1755, 608), (1813, 689)), c=0.5903230905532837), TrackedFrame(f=14, bb=((1755, 606), (1814, 692)), c=0.6314746737480164), TrackedFrame(f=15, bb=((1766, 610), (1824, 694)), c=0.5695499181747437), TrackedFrame(f=16, bb=((1772, 610), (1827, 693)), c=0.7277531027793884), TrackedFrame(f=17, bb=((1775, 609), (1831, 696)), c=0.709379255771637), TrackedFrame(f=18, bb=((1778, 610), (1839, 695)), c=0.7195687890052795), TrackedFrame(f=19, bb=((1778, 609), (1848, 694)), c=0.6800578832626343), TrackedFrame(f=22, bb=((1793, 612), (1865, 695)), c=0.627274751663208), TrackedFrame(f=23, bb=((1800, 612), (1871, 696)), c=0.7736772298812866), TrackedFrame(f=24, bb=((1803, 613), (1874, 697)), c=0.7796608209609985), TrackedFrame(f=25, bb=((1805, 613), (1878, 694)), c=0.7743455171585083), TrackedFrame(f=26, bb=((1808, 617), (1884, 701)), c=0.7462272644042969), TrackedFrame(f=27, bb=((1813, 619), (1890, 699)), c=0.7425493597984314), TrackedFrame(f=28, bb=((1816, 619), (1891, 703)), c=0.751545250415802), TrackedFrame(f=29, bb=((1819, 619), (1895, 703)), c=0.7550487518310547), TrackedFrame(f=30, bb=((1824, 620), (1897, 698)), c=0.7756643891334534), TrackedFrame(f=31, bb=((1825, 619), (1900, 702)), c=0.7463307976722717), TrackedFrame(f=32, bb=((1827, 618), (1903, 703)), c=0.7050273418426514), TrackedFrame(f=33, bb=((1828, 618), (1905, 703)), c=0.6891172528266907), TrackedFrame(f=34, bb=((1831, 617), (1906, 703)), c=0.7078866958618164), TrackedFrame(f=35, bb=((1835, 617), (1910, 705)), c=0.7494577169418335), TrackedFrame(f=36, bb=((1837, 618), (1911, 707)), c=0.7472692131996155), TrackedFrame(f=37, bb=((1841, 619), (1914, 706)), c=0.7334573864936829), TrackedFrame(f=38, bb=((1844, 619), (1916, 703)), c=0.7418296337127686), TrackedFrame(f=39, bb=((1851, 622), (1918, 700)), c=0.6811541318893433), TrackedFrame(f=40, bb=((1853, 622), (1918, 704)), c=0.7226799726486206), TrackedFrame(f=41, bb=((1854, 623), (1919, 708)), c=0.7388505935668945), TrackedFrame(f=42, bb=((1857, 623), (1919, 713)), c=0.7644845843315125), TrackedFrame(f=43, bb=((1861, 623), (1920, 714)), c=0.7426022291183472), TrackedFrame(f=44, bb=((1862, 625), (1919, 716)), c=0.7738450765609741), TrackedFrame(f=45, bb=((1866, 630), (1919, 722)), c=0.7585200071334839), TrackedFrame(f=46, bb=((1866, 628), (1919, 723)), c=0.7446922063827515), TrackedFrame(f=47, bb=((1871, 630), (1919, 722)), c=0.7594582438468933), TrackedFrame(f=48, bb=((1873, 631), (1920, 722)), c=0.7599528431892395), TrackedFrame(f=49, bb=((1874, 633), (1920, 722)), c=0.7090628147125244), TrackedFrame(f=50, bb=((1876, 634), (1920, 728)), c=0.5804612040519714), TrackedFrame(f=51, bb=((1875, 633), (1919, 722)), c=0.5937541723251343)]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from gorillatracker.utils.video_models import VideoClip, _parse_tracked_video_clip\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "json_path = \"/workspaces/gorillatracker/data/derived_data/spac_gorillas_converted_labels_tracked/M002_20220529_031_tracked.json\"\n",
    "mp4_path = \"/workspaces/gorillatracker/video_data\" + json_path.split(\"spac_gorillas_converted_labels_tracked\")[1].replace(\"_tracked.json\", \".mp4\")\n",
    "print(mp4_path)\n",
    "v = VideoClip(video_id=\"\", camera_id=\"\", start_time=datetime.now())\n",
    "v = _parse_tracked_video_clip(v, json_path)\n",
    "video = cv2.VideoCapture(mp4_path)\n",
    "\n",
    "for i, _ in enumerate(v.trackings):\n",
    "    gorilla = v.trackings[i]\n",
    "    if len(gorilla.bounding_boxes_face) > 0:\n",
    "        print(i)\n",
    "        print(gorilla.bounding_boxes_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract face images and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gorilla = v.trackings[0] # change this to the gorilla you want to extract the embeddings from\n",
    "\n",
    "faces = []\n",
    "embeddings = []\n",
    "for frame in gorilla.bounding_boxes_face:\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, frame.f)\n",
    "    ret, img = video.read()\n",
    "    cropped_img = Image.fromarray(img).crop(frame.bb[0] + frame.bb[1])\n",
    "    faces.append(cropped_img)\n",
    "    img = transform(cropped_img)\n",
    "    img = img.unsqueeze(0)\n",
    "    embedding = model(img)\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Embedding Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap, LocallyLinearEmbedding, MDS, SpectralEmbedding, TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "class EmbeddingProjector:\n",
    "    def __init__(self):\n",
    "        self.algorithms = {\n",
    "            \"tsne\": TSNE(n_components=2),\n",
    "            \"isomap\": Isomap(n_components=2),\n",
    "            \"lle\": LocallyLinearEmbedding(n_components=2),\n",
    "            \"mds\": MDS(n_components=2),\n",
    "            \"spectral\": SpectralEmbedding(n_components=2),\n",
    "            \"pca\": PCA(n_components=2),\n",
    "            \"umap\": umap.UMAP(),\n",
    "        }\n",
    "\n",
    "    def reduce_dimensions(self, embeddings, method=\"tsne\"):\n",
    "        algorithm = self.algorithms.get(method, TSNE(n_components=2))\n",
    "        return algorithm.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot each embedding with a slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6.069356441497803, -2.606229543685913],\n",
       " [5.707740306854248, -0.13445377349853516],\n",
       " [5.233815670013428, -2.5183310508728027],\n",
       " [4.776072978973389, -3.5770421028137207],\n",
       " [4.561035633087158, -3.3034815788269043],\n",
       " [2.3291049003601074, -3.09199595451355],\n",
       " [3.0421857833862305, -1.7251754999160767],\n",
       " [2.613830804824829, -2.098525285720825],\n",
       " [1.4418076276779175, -1.5222442150115967],\n",
       " [0.9208231568336487, -1.4479016065597534],\n",
       " [0.02792559191584587, -2.535956382751465],\n",
       " [0.042257774621248245, -2.443587064743042],\n",
       " [-0.5683120489120483, -1.1450175046920776],\n",
       " [-0.4032926857471466, -2.3702445030212402],\n",
       " [-1.182234287261963, -3.036203622817993],\n",
       " [-4.032674789428711, -1.3732041120529175],\n",
       " [-3.8200595378875732, -1.6110939979553223],\n",
       " [-3.826831579208374, -2.099912166595459],\n",
       " [-3.755432367324829, -0.9303005337715149],\n",
       " [-3.696626663208008, -1.093754529953003],\n",
       " [-3.7225329875946045, -1.333591103553772],\n",
       " [-3.424424409866333, -1.491300106048584],\n",
       " [-3.858016014099121, -0.698914647102356],\n",
       " [-3.9881627559661865, -0.1936197727918625],\n",
       " [-5.23950719833374, 1.2331305742263794],\n",
       " [-4.87797212600708, 3.700032949447632],\n",
       " [-4.495335102081299, 2.6507105827331543],\n",
       " [-2.9643311500549316, 1.9434341192245483],\n",
       " [-2.9050910472869873, 2.403933525085449],\n",
       " [-2.922551393508911, 2.4380578994750977],\n",
       " [-0.8023793697357178, 5.283851623535156],\n",
       " [1.6958093643188477, 4.726972579956055],\n",
       " [4.568154811859131, 5.366671085357666],\n",
       " [6.03170919418335, 4.605550765991211],\n",
       " [3.7887470722198486, 5.433531761169434],\n",
       " [3.7112784385681152, 3.968736171722412],\n",
       " [3.924114465713501, 0.6274752020835876]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b26536272944b9eb7c684eb896061af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Page:', max=36), Output()), _dom_classes=('widget-interaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_images(page)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO\n",
    "\n",
    "low_dim_embeddings = EmbeddingProjector().reduce_dimensions(torch.cat(embeddings).detach().numpy(),\n",
    "                                                            method=\"pca\")\n",
    "low_dim_embeddings = low_dim_embeddings.tolist()\n",
    "display(low_dim_embeddings)\n",
    "\n",
    "x_axis, y_axis = zip(*low_dim_embeddings)\n",
    "plot_list = []\n",
    "for embedding in low_dim_embeddings:\n",
    "    plt.xlim(min(x_axis) - 1, max(x_axis) + 1)\n",
    "    plt.ylim(min(y_axis) - 1, max(y_axis) + 1)\n",
    "    plt.plot(embedding[0], embedding[1], marker='+', linestyle='None', markersize=10, color='blue')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    pil_image = Image.open(buffer)\n",
    "    plot_list.append(pil_image)\n",
    "    plt.close()\n",
    "\n",
    "images_per_page = 2\n",
    "\n",
    "def display_images(page):\n",
    "    start = page\n",
    "    print(start)\n",
    "    fig, axs = plt.subplots(1, images_per_page, figsize=(15, 5))  # Create subplots\n",
    "    for i in range(images_per_page):\n",
    "        if start + i < len(faces) + len(plot_list):\n",
    "            if i == 0:\n",
    "                axs[i].imshow(faces[start])\n",
    "            else:\n",
    "                axs[i].imshow(plot_list[start])\n",
    "            axs[i].axis('off')\n",
    "        else:\n",
    "            axs[i].axis('off')  # Hide axes for empty subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "page_selector = widgets.IntSlider(min=0, max=(len(faces) + len(plot_list) - 1) // images_per_page, description='Page:')\n",
    "widgets.interact(display_images, page=page_selector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
