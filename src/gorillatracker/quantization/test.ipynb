{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 09:09:13.475445: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=1\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717060154.188171  752003 service.cc:145] XLA service 0x55749cb8bcc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1717060154.188219  752003 service.cc:153]   StreamExecutor device (0): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1717060154.188875  752003 se_gpu_pjrt_client.cc:853] Using BFC allocator.\n",
      "I0000 00:00:1717060154.188915  752003 gpu_helpers.cc:114] XLA backend allocating 63707234304 bytes on device 0 for BFCAllocator.\n",
      "I0000 00:00:1717060154.188944  752003 gpu_helpers.cc:154] XLA backend will use up to 21235744768 bytes on device 0 for CollectiveBFCAllocator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='https', netloc='wandb.ai', path='/gorillas/Embedding-EfficientNet-CXL-OpenSet/runs/famq71r6/workspace', params='', query='nw=nwuserkajohpi', fragment='') ['', 'gorillas', 'Embedding-EfficientNet-CXL-OpenSet', 'runs', 'famq71r6', 'workspace'] /gorillas/Embedding-EfficientNet-CXL-OpenSet/runs/famq71r6/workspace\n",
      "Using model from run: 261-add-the-ability-to-do-quantization-using-pytorch-2-export-quantization-2024-05-21-07-13-45\n",
      "Config: {'s': 64, 'seed': 42, 'beta1': 0.9, 'beta2': 0.999, 'debug': False, 'kfold': False, 'end_lr': 1e-06, 'margin': 1, 'resume': False, 'compile': False, 'delta_t': 50, 'epsilon': 1e-07, 'l2_beta': 0.01, 'offline': False, 'plugins': None, 'use_ssl': False, 'workers': 16, 'data_dir': '/workspaces/gorillatracker/data/splits/ground_truth-cxl-face_images-openset-reid-val-0-test-0-mintraincount-3-seed-42-train-50-val-25-test-25', 'l2_alpha': 0.1, 'only_val': False, 'profiler': None, 'run_name': '261-add-the-ability-to-do-quantization-using-pytorch-2-export-quantization', 'start_lr': 0.0001, 'dropout_p': 0.32, 'grad_clip': 1, 'loss_mode': 'online/soft', 'min_delta': 0.01, 'precision': 32, 'batch_size': 8, 'initial_lr': 0.0001, 'max_epochs': 30, 'wandb_tags': [], 'accelerator': 'cuda', 'lr_interval': 1, 'lr_schedule': 'exponential', 'num_classes': [59, 29, 29], 'num_devices': 1, 'warmup_mode': 'exponential', 'fast_dev_run': False, 'from_scratch': False, 'project_name': 'Embedding-EfficientNet-CXL-OpenSet', 'weight_decay': 0.5, 'dataset_class': 'gorillatracker.datasets.cxl.CXLDataset', 'save_interval': 25, 'warmup_epochs': 0, 'embedding_size': 256, 'knn_with_train': True, 'lambda_membank': 0.5, 'use_wildme_model': False, 'stepwise_schedule': False, 'model_name_or_path': 'EfficientNetV2_Large', 'val_check_interval': 1, 'force_deterministic': True, 'save_model_to_wandb': True, 'val_before_training': True, 'distributed_strategy': 'auto', 'mem_bank_start_epoch': 10, 'data_resize_transform': 224, 'saved_checkpoint_path': None, 'check_val_every_n_epoch': 1, 'data_preprocessing_only': False, 'early_stopping_patience': 3, 'embedding_save_interval': 5, 'pretrained_weights_file': None, 'path_to_pretrained_weights': './models/swin_base_untrained.ckpt', 'gradient_accumulation_steps': 2, 'use_quantization_aware_training': False}\n",
      "Loading model from latest checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-famq71r6:v0, 1348.64MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:2.3\n",
      "W0530 09:09:21.578000 140652183078720 torch/_export/__init__.py:94] +============================+\n",
      "W0530 09:09:21.579000 140652183078720 torch/_export/__init__.py:95] |     !!!   WARNING   !!!    |\n",
      "W0530 09:09:21.579000 140652183078720 torch/_export/__init__.py:96] +============================+\n",
      "W0530 09:09:21.580000 140652183078720 torch/_export/__init__.py:97] capture_pre_autograd_graph() is deprecated and doesn't provide any function guarantee moving forward.\n",
      "W0530 09:09:21.580000 140652183078720 torch/_export/__init__.py:98] Please switch to use torch.export instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared\n",
      "Converted\n"
     ]
    }
   ],
   "source": [
    "from gorillatracker.datasets.cxl import CXLDataset\n",
    "from gorillatracker.model import BaseModule\n",
    "from gorillatracker.quantization.utils import get_model_input\n",
    "from gorillatracker.utils.embedding_generator import get_model_for_run_url\n",
    "\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\n",
    "from torch._export import capture_pre_autograd_graph\n",
    "import torch.ao.quantization\n",
    "\n",
    "from ai_edge_torch.quantize.pt2e_quantizer import get_symmetric_quantization_config\n",
    "from ai_edge_torch.quantize.pt2e_quantizer import PT2EQuantizer\n",
    "from ai_edge_torch.quantize.quant_config import QuantConfig\n",
    "\n",
    "from ai_edge_torch.debug import find_culprits\n",
    "\n",
    "save_quantized_model = False\n",
    "load_quantized_model = False\n",
    "save_model_architecture = False\n",
    "number_of_calibration_images = 100\n",
    "dataset_path = \"/workspaces/gorillatracker/data/splits/ground_truth-cxl-face_images-openset-reid-val-0-test-0-mintraincount-3-seed-42-train-50-val-25-test-25\"\n",
    "model_wandb_url = (\n",
    "    \"https://wandb.ai/gorillas/Embedding-EfficientNet-CXL-OpenSet/runs/famq71r6/workspace?nw=nwuserkajohpi\"\n",
    ")\n",
    "\n",
    "\n",
    "# 1. Quantization\n",
    "\n",
    "calibration_input_embeddings, _ = get_model_input(\n",
    "    CXLDataset, dataset_path=dataset_path, partion=\"train\", amount_of_tensors=number_of_calibration_images\n",
    ")\n",
    "model: BaseModule = get_model_for_run_url(model_wandb_url)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "pt2e_quantizer = PT2EQuantizer().set_global(get_symmetric_quantization_config(is_per_channel=True, is_dynamic=True))\n",
    "\n",
    "pt2e_torch_model = capture_pre_autograd_graph(model, (calibration_input_embeddings,))\n",
    "pt2e_torch_model = prepare_pt2e(pt2e_torch_model, pt2e_quantizer)\n",
    "\n",
    "print(\"Prepared\")\n",
    "\n",
    "# Run the prepared model with sample input data to ensure that internal observers are populated with correct values\n",
    "pt2e_torch_model(*(calibration_input_embeddings,))\n",
    "\n",
    "# Convert the prepared model to a quantized model\n",
    "pt2e_torch_model = convert_pt2e(pt2e_torch_model, fold_quantize=False)\n",
    "torch.ao.quantization.allow_exported_model_train_eval(pt2e_torch_model)\n",
    "print(\"Converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(model(calibration_input_embeddings))\n",
    "# print(pt2e_torch_model(calibration_input_embeddings))\n",
    "\n",
    "calibration_input_embeddings[0].unsqueeze(0).shape\n",
    "# culprits = find_culprits(pt2e_torch_model, (calibration_input_embeddings,))\n",
    "# culprit = next(culprits)\n",
    "# culprit.print_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/research/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:361: UserWarning: At pre-dispatch tracing, we will assume that any custom op that is marked with CompositeImplicitAutograd and functional are safe to not decompose. We found quantized_decomposed.quantize_per_channel.default to be one such op.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/research/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:361: UserWarning: At pre-dispatch tracing, we will assume that any custom op that is marked with CompositeImplicitAutograd and functional are safe to not decompose. We found quantized_decomposed.dequantize_per_channel.default to be one such op.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import torch.ao.quantization.pt2e.export_utils\n",
    "# torch.ao.quantization.pt2e.export_utils.model_is_exported(pt2e_torch_model)\n",
    "\n",
    "# quantized_ep = torch.export.export(pt2e_torch_model, (calibration_input_embeddings,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Your model is converted in training mode. Please set the module in evaluation mode with `module.eval()` for better on-device performance and compatibility.\n",
      "/opt/conda/envs/research/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:361: UserWarning: At pre-dispatch tracing, we will assume that any custom op that is marked with CompositeImplicitAutograd and functional are safe to not decompose. We found quantized_decomposed.quantize_per_channel.default to be one such op.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/research/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:361: UserWarning: At pre-dispatch tracing, we will assume that any custom op that is marked with CompositeImplicitAutograd and functional are safe to not decompose. We found quantized_decomposed.dequantize_per_channel.default to be one such op.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1717061088.589067  752003 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1717061088.589090  752003 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n"
     ]
    }
   ],
   "source": [
    "import ai_edge_torch\n",
    "pt2e_drq_model = ai_edge_torch.convert(\n",
    "    pt2e_torch_model, (calibration_input_embeddings[0].unsqueeze(0),), quant_config=QuantConfig(pt2e_quantizer=pt2e_quantizer)\n",
    ")\n",
    "pt2e_drq_model.export(\"quantized_model.tflite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
